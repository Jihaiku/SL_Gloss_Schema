{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506898ee-f26d-408d-9bc3-6f0c5c584c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Model Setup: Starting =====\n",
      "\n",
      "Loading base Whisper model with 8-bit quantization...\n",
      "binary_path: C:\\Users\\cdhye\\anaconda3\\envs\\ASR\\lib\\site-packages\\bitsandbytes\\cuda_setup\\libbitsandbytes_cuda116.dll\n",
      "Model loaded successfully.\n",
      "\n",
      "Applying LoRA configuration...\n",
      "LoRA applied successfully.\n",
      "\n",
      "===== LoRA Configuration =====\n",
      "LoRA Rank (r): 32\n",
      "LoRA Alpha: 64\n",
      "LoRA Dropout: 0.05\n",
      "LoRA Target Modules: ['q_proj', 'v_proj']\n",
      "LoRA Bias: none \n",
      "\n",
      "Trainable parameter summary:\n",
      "trainable params: 589,824 || all params: 38,350,464 || trainable%: 1.5380\n",
      "\n",
      "===== Model Setup: Complete =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Imports\n",
    "#\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from transformers import WhisperForConditionalGeneration, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "from config_0 import load_config\n",
    "from dataset_1 import load_and_prepare_dataset, load_processors\n",
    "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "#\n",
    "# Load configuration\n",
    "#\n",
    "config = load_config()\n",
    "\n",
    "rand_num = random.randint(1,200)\n",
    "seed = rand_num\n",
    "csv_filename = \"model_results.csv\"\n",
    "model_name_or_path = config[\"model_name_or_path\"]\n",
    "model = config[\"model\"]\n",
    "language = config[\"language\"]\n",
    "language_abbr = config[\"language_abbr\"]\n",
    "task = config[\"task\"]\n",
    "dataset_name = config[\"dataset_name\"]\n",
    "size = config[\"size\"]\n",
    "user_name = config[\"user_name\"]\n",
    "peft_type = config[\"peft_type\"]\n",
    "output_dir = f\"{model_name_or_path}-{language_abbr}-{size}-{seed}\"\n",
    "\n",
    "#\n",
    "# Load base Whisper model with 8-bit quantization\n",
    "#\n",
    "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "def load_quantized_whisper_model():\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(\n",
    "        model_name_or_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    model.config.forced_decoder_ids = None\n",
    "    model.config.suppress_tokens = []\n",
    "    return model\n",
    "\n",
    "#\n",
    "# Apply PEFT LoRA configuration\n",
    "#\n",
    "def apply_lora(model):\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=config[\"r\"],\n",
    "        lora_alpha=config[\"lora_alpha\"],\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        lora_dropout=config[\"lora_dropout\"],\n",
    "        bias=config[\"bias\"]\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    return model\n",
    "\n",
    "#\n",
    "# Main execution (for standalone testing)\n",
    "#\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n===== Model Setup: Starting =====\\n\")\n",
    "    print(\"Loading base Whisper model with 8-bit quantization...\")\n",
    "    model = load_quantized_whisper_model()\n",
    "    print(\"Model loaded successfully.\\n\")\n",
    "\n",
    "    print(\"Applying LoRA configuration...\")\n",
    "    model = apply_lora(model)\n",
    "    print(\"LoRA applied successfully.\\n\")\n",
    "    print(\"===== LoRA Configuration =====\")\n",
    "    print(f\"LoRA Rank (r): {config['r']}\")\n",
    "    print(f\"LoRA Alpha: {config['lora_alpha']}\")\n",
    "    print(f\"LoRA Dropout: {config['lora_dropout']}\")\n",
    "    print(f\"LoRA Target Modules: ['q_proj', 'v_proj']\")\n",
    "    print(f\"LoRA Bias: {config['bias']} \\n\")\n",
    "\n",
    "    print(\"Trainable parameter summary:\")\n",
    "    model.print_trainable_parameters()\n",
    "    print(\"\\n===== Model Setup: Complete =====\\n\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
