{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a08e4b6b-e30d-4f85-884d-ad8d1f51102f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. Loading Config\n",
      "\n",
      "===== Config Info =====\n",
      "Config contents:\n",
      "\n",
      "model_name_or_path : openai/whisper-tiny\n",
      "model              : whisper-tiny\n",
      "language           : English\n",
      "language_abbr      : en\n",
      "task               : transcribe\n",
      "dataset_name       : mozilla-foundation/common_voice_11_0\n",
      "size               : 0.0005\n",
      "user_name          : jihaiku\n",
      "peft_type          : LoRA\n",
      "r                  : 32\n",
      "lora_alpha         : 64\n",
      "lora_dropout       : 0.05\n",
      "bias               : none\n",
      "csv_path           : active\n",
      "json_path          : None\n",
      "===== Config Info ===== \n",
      "\n",
      "0. Configuration loaded.\n",
      "\n",
      "1. Loading Dataset\n",
      "\n",
      "===== Datset Info =====\n",
      "Sampling rate: 16000 Hz\n",
      "Dataset Name: mozilla-foundation/common_voice_11_0\n",
      "Percentage of the dataset: 0.0005%\n",
      "Split: train\n",
      "  Number of samples: 474\n",
      "Split: test\n",
      "  Number of samples: 8\n",
      "===== Dataset Info =====\n",
      "\n",
      "1. Dataset loaded and prepared.\n",
      "\n",
      "2. Loading Model\n",
      "\n",
      "===== Model Info =====\n",
      "Loading base Whisper model with 8-bit quantization...\n",
      "Model loaded successfully.\n",
      "\n",
      "Applying LoRA configuration...\n",
      "LoRA applied successfully.\n",
      "\n",
      "===== LoRA Configuration =====\n",
      "LoRA Rank (r): 32\n",
      "LoRA Alpha: 64\n",
      "LoRA Dropout: 0.05\n",
      "LoRA Target Modules: ['q_proj', 'v_proj']\n",
      "LoRA Bias: none \n",
      "\n",
      "Trainable parameter summary:\n",
      "trainable params: 589,824 || all params: 38,350,464 || trainable%: 1.5380\n",
      "===== Model Info =====\n",
      "\n",
      "2. Model loaded and LoRA applied.\n",
      "\n",
      "Loading model...\n",
      "\n",
      "Training samples: 474\n",
      "Evaluation samples: 8\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 02:54, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.945072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>1.943352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>0.998694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000001E534048370>>\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\cdhye\\anaconda3\\envs\\ASR\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 790, in _clean_thread_parent_frames\n",
      "    active_threads = {thread.ident for thread in threading.enumerate()}\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import gc\n",
    "import warnings\n",
    "\n",
    "from datasets import Audio\n",
    "from transformers import WhisperProcessor, WhisperTokenizer, WhisperFeatureExtractor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from config_0 import load_config\n",
    "from dataset_1 import load_and_prepare_dataset, load_processors\n",
    "from model_2 import load_quantized_whisper_model, apply_lora\n",
    "from trainer_3 import train_model, DataCollatorSpeechSeq2SeqWithPadding\n",
    "from eval_4 evaluate_model_wer, log_results_to_csv\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "def main(verbose=True):\n",
    "    # Step 0: Load configuration\n",
    "    print(\"0. Loading Config\")\n",
    "    config = load_config(\"config.yaml\")\n",
    "    model_name_or_path = config[\"model_name_or_path\"]\n",
    "    language = config[\"language\"]\n",
    "    language_abbr = config[\"language_abbr\"]\n",
    "    task = config[\"task\"]\n",
    "    dataset_name = config[\"dataset_name\"]\n",
    "    size = config[\"size\"]\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    os.environ[\"HF_HOME\"] = os.getcwd()\n",
    "    os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\"\n",
    "\n",
    "    seed = random.randint(1, 200)\n",
    "    csv_filename = \"model_results.csv\"\n",
    "    output_dir = f\"{config['model_name_or_path']}-{config['language_abbr']}-{config['size']}-{seed}\"\n",
    "\n",
    "    if verbose == True:\n",
    "        try:\n",
    "            cfg = load_config()\n",
    "            assert isinstance(cfg, dict), \"Config is not a dictionary.\"\n",
    "            print(\"\\n===== Config Info =====\")\n",
    "            print(\"Config contents:\\n\")\n",
    "            max_key_len = max(len(key) for key in cfg.keys())\n",
    "            for key, value in cfg.items():\n",
    "                print(f\"{key.ljust(max_key_len)} : {value}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Test failed: {e}\")\n",
    "        print(\"===== Config Info ===== \")\n",
    "    \n",
    "    \n",
    "    print(\"\\n0. Configuration loaded.\")\n",
    "\n",
    "\n",
    "    # Step 1: Load and prepare dataset\n",
    "    print(\"\\n1. Loading Dataset\")\n",
    "    dataset = load_and_prepare_dataset()\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"\\n===== Datset Info =====\")\n",
    "        dataset = load_and_prepare_dataset()\n",
    "    \n",
    "        # Print sampling rate\n",
    "        sample_audio = dataset[\"train\"][0][\"audio\"]\n",
    "        print(f\"Sampling rate: {sample_audio['sampling_rate']} Hz\")\n",
    "        print(f\"Dataset Name: {dataset_name}\")\n",
    "        print(f\"Percentage of the dataset: {size}%\")\n",
    "        for split in [\"train\", \"test\"]:\n",
    "            print(f\"Split: {split}\")\n",
    "            print(f\"  Number of samples: {len(dataset[split])}\")\n",
    "        print(\"===== Dataset Info =====\")\n",
    "        \n",
    "    print(\"\\n1. Dataset loaded and prepared.\")\n",
    "\n",
    "    \n",
    "    # Step 2: Load quantized Whisper model & apply LoRA\n",
    "    print(\"\\n2. Loading Model\")\n",
    "    base_model = load_quantized_whisper_model()\n",
    "    model = apply_lora(base_model)\n",
    "\n",
    "    if verbose == True:\n",
    "        print(\"\\n===== Model Info =====\")\n",
    "        print(\"Loading base Whisper model with 8-bit quantization...\")\n",
    "        model = load_quantized_whisper_model()\n",
    "        print(\"Model loaded successfully.\\n\")\n",
    "    \n",
    "        print(\"Applying LoRA configuration...\")\n",
    "        model = apply_lora(model)\n",
    "        print(\"LoRA applied successfully.\\n\")\n",
    "    \n",
    "        print(\"===== LoRA Configuration =====\")\n",
    "        print(f\"LoRA Rank (r): {config['r']}\")\n",
    "        print(f\"LoRA Alpha: {config['lora_alpha']}\")\n",
    "        print(f\"LoRA Dropout: {config['lora_dropout']}\")\n",
    "        print(f\"LoRA Target Modules: ['q_proj', 'v_proj']\")\n",
    "        print(f\"LoRA Bias: {config['bias']} \\n\")\n",
    "    \n",
    "        print(\"Trainable parameter summary:\")\n",
    "        model.print_trainable_parameters()\n",
    "        print(\"===== Model Info =====\\n\")   \n",
    "    \n",
    "    print(\"2. Model loaded and LoRA applied.\")\n",
    "\n",
    "\n",
    "    # Step 3: Train\n",
    "    print(\"\\nLoading model...\")\n",
    "    base_model = load_quantized_whisper_model()\n",
    "    model = apply_lora(base_model)\n",
    "    \n",
    "    dataset = load_and_prepare_dataset()\n",
    "    feature_extractor, tokenizer, processor = load_processors(model_name_or_path, language, task)\n",
    "    \n",
    "    prepare_dataset = get_prepare_dataset_fn(feature_extractor, tokenizer)\n",
    "    dataset = dataset.map(prepare_dataset, remove_columns=[\"audio\", \"sentence\"])\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor)\n",
    "\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "\n",
    "    print(f\"\\nTraining samples: {len(train_dataset)}\")\n",
    "    print(f\"Evaluation samples: {len(eval_dataset)}\\n\")\n",
    "\n",
    "   # Train\n",
    "    train_model(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        output_dir=output_dir,\n",
    "        seed=seed,\n",
    "        csv_filename=csv_filename\n",
    "    )\n",
    "\n",
    "    # if verbose == True:\n",
    "    #     print(\"\\n=== Training Arguments ===\")\n",
    "    #     print(f\"output_dir: {training_args.output_dir}\")\n",
    "    #     print(f\"per_device_train_batch_size: {training_args.per_device_train_batch_size}\")\n",
    "    #     print(f\"gradient_accumulation_steps: {training_args.gradient_accumulation_steps}\")\n",
    "    #     print(f\"learning_rate: {training_args.learning_rate}\")\n",
    "    #     print(f\"warmup_steps: {training_args.warmup_steps}\")\n",
    "    #     print(f\"num_train_epochs: {training_args.num_train_epochs}\")\n",
    "    #     print(f\"eval_strategy: {training_args.evaluation_strategy}\")\n",
    "    #     print(f\"per_device_eval_batch_size: {training_args.per_device_eval_batch_size}\")\n",
    "    #     print(f\"generation_max_length: {training_args.generation_max_length}\")\n",
    "    #     print(f\"logging_steps: {training_args.logging_steps}\")\n",
    "    #     print(f\"save_total_limit: {training_args.save_total_limit}\")\n",
    "    #     print(\"=== Training Arguments ===\")\n",
    "\n",
    "    # print(\"3. Training completed.\")\n",
    "\n",
    "    # Step 4: Evaluate\n",
    "    wer, normalized_wer = evaluate_model_wer(model, tokenizer, dataset[\"test\"], data_collator)\n",
    "    print(f\"WER: {wer:.2f}%, Normalized WER: {normalized_wer:.4f}%\")\n",
    "\n",
    "    # Step 5: Log\n",
    "    wer, normalized_wer = evaluate_model_wer(model, tokenizer, dataset[\"test\"], data_collator)\n",
    "    log_results_to_csv(wer, normalized_wer, csv_filename=\"eval_test.csv\")\n",
    "\n",
    "    # gc.collect()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca338df-fb10-4688-a57d-d4f96fe98918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
