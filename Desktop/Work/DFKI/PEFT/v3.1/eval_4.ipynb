{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91093d2-f1bb-40d7-b453-0e91a029348a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='444' max='444' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [444/444 27:31, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.512100</td>\n",
       "      <td>0.631930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.442900</td>\n",
       "      <td>0.641789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER: 30.11%, Normalized WER: 4.2174%\n",
      "+--------+-------+------------+-----------------+-------------+-----+--------------+----------------+--------+----------------------+--------------+------------+-----------------+------------+\n",
      "|   Seed |   WER |   Norm_WER |   Training_Size |   Test_Size |   r |   lora_alpha |   lora_dropout | bias   | model_name_or_path   | model        | language   | language_abbr   | task       |\n",
      "+========+=======+============+=================+=============+=====+==============+================+========+======================+==============+============+=================+============+\n",
      "|    164 | 49.28 |      71.41 |             474 |           8 |  32 |           64 |           0.05 | none   | openai/whisper-tiny  | whisper-tiny | English    | en              | transcribe |\n",
      "+--------+-------+------------+-----------------+-------------+-----+--------------+----------------+--------+----------------------+--------------+------------+-----------------+------------+\n",
      "|    164 | 46.38 |      67.21 |             474 |           8 |  32 |           64 |           0.05 | none   | openai/whisper-tiny  | whisper-tiny | English    | en              | transcribe |\n",
      "+--------+-------+------------+-----------------+-------------+-----+--------------+----------------+--------+----------------------+--------------+------------+-----------------+------------+\n",
      "|    171 | 36.71 |      46.47 |             474 |           8 |  32 |           64 |           0.05 | none   | openai/whisper-tiny  | whisper-tiny | English    | en              | transcribe |\n",
      "+--------+-------+------------+-----------------+-------------+-----+--------------+----------------+--------+----------------------+--------------+------------+-----------------+------------+\n",
      "|    155 | 30.67 |       4.3  |            4743 |          81 |  32 |           64 |           0.05 | none   | openai/whisper-tiny  | whisper-tiny | English    | en              | transcribe |\n",
      "+--------+-------+------------+-----------------+-------------+-----+--------------+----------------+--------+----------------------+--------------+------------+-----------------+------------+\n",
      "|    155 | 30.11 |       4.22 |            4743 |          81 |  32 |           64 |           0.05 | none   | openai/whisper-tiny  | whisper-tiny | English    | en              | transcribe |\n",
      "+--------+-------+------------+-----------------+-------------+-----+--------------+----------------+--------+----------------------+--------------+------------+-----------------+------------+\n",
      "|    155 | 30.11 |       4.22 |            4743 |          81 |  32 |           64 |           0.05 | none   | openai/whisper-tiny  | whisper-tiny | English    | en              | transcribe |\n",
      "+--------+-------+------------+-----------------+-------------+-----+--------------+----------------+--------+----------------------+--------------+------------+-----------------+------------+\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import evaluate\n",
    "import gc\n",
    "import jiwer\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "from datasets import load_metric\n",
    "from tabulate import tabulate\n",
    "from transformers import Seq2SeqTrainer\n",
    "from config_0 import load_config\n",
    "from dataset_1 import load_and_prepare_dataset, get_prepare_dataset_fn, load_processors\n",
    "from model_2 import load_quantized_whisper_model, apply_lora\n",
    "from trainer_3_1 import train_model, DataCollatorSpeechSeq2SeqWithPadding\n",
    "\n",
    "\n",
    "config = load_config()\n",
    "\n",
    "model_name_or_path = config[\"model_name_or_path\"]\n",
    "model = config[\"model\"]\n",
    "language = config[\"language\"]\n",
    "language_abbr = config[\"language_abbr\"]\n",
    "task = config[\"task\"]\n",
    "dataset_name = config[\"dataset_name\"]\n",
    "size = config[\"size\"]\n",
    "user_name = config[\"user_name\"]\n",
    "peft_type = config[\"peft_type\"]\n",
    "\n",
    "csv_filename = \"model_results.csv\"\n",
    "rand_num = random.randint(1,200)\n",
    "seed = 155\n",
    "output_dir = f\"{model_name_or_path}-{language_abbr}-{size}-{seed}\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import gc\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "def evaluate_model_wer(model, tokenizer, test_dataset, data_collator, batch_size=8):\n",
    "    eval_dataloader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=data_collator)\n",
    "    model.eval()\n",
    "    metric = evaluate.load(\"wer\")\n",
    "    \n",
    "    total_reference_tokens = 0\n",
    "    \n",
    "    for step, batch in enumerate(tqdm(eval_dataloader, desc=\"Evaluating\", disable=True)):\n",
    "        with torch.amp.autocast(\"cuda\"), torch.no_grad():\n",
    "            generated_tokens = (\n",
    "                model.generate(\n",
    "                    input_features=batch[\"input_features\"].to(\"cuda\"),\n",
    "                    decoder_input_ids=batch[\"labels\"][:, :4].to(\"cuda\"),\n",
    "                    max_new_tokens=255,\n",
    "                )\n",
    "                .cpu()\n",
    "                .numpy()\n",
    "            )\n",
    "    \n",
    "            labels = batch[\"labels\"].cpu().numpy()\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            \n",
    "            decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n",
    "            total_reference_tokens += sum(len(label.split()) for label in decoded_labels)\n",
    "    \n",
    "    wer = 100 * metric.compute()\n",
    "    normalized_wer = (wer / total_reference_tokens) * 100 if total_reference_tokens > 0 else None\n",
    "    gc.collect()\n",
    "    \n",
    "    return wer, normalized_wer\n",
    "\n",
    "def log_results_to_csv(wer, normalized_wer, dataset, seed, config_path=\"config.yaml\", csv_filename=\"model_results.csv\"):\n",
    "    config = load_config()\n",
    "\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = config.get(\"cuda_visible_devices\", \"0\")\n",
    "\n",
    "    wer = round(float(wer), 2)\n",
    "    normalized_wer = round(float(normalized_wer), 2)\n",
    "\n",
    "    lora_keys = [\"r\", \"lora_alpha\", \"lora_dropout\", \"bias\"]\n",
    "    model_keys = [\"model_name_or_path\", \"model\", \"language\", \"language_abbr\", \"task\"]\n",
    "\n",
    "    csv_headers = [\"Seed\",\"WER\", \"Norm_WER\", \"Training_Size\", \"Test_Size\",] + lora_keys + model_keys\n",
    "\n",
    "    csv_data = [seed, wer, normalized_wer, len(dataset[\"train\"]), len(dataset[\"test\"]), ] + [config.get(key, \"NA\") for key in lora_keys] + [config[key] for key in model_keys]\n",
    "\n",
    "    write_headers = not os.path.exists(csv_filename) or os.path.getsize(csv_filename) == 0\n",
    "\n",
    "    with open(csv_filename, mode=\"a\", newline=\"\") as file:\n",
    "        writer = csv.writer(file)\n",
    "    \n",
    "        if write_headers:\n",
    "            writer.writerow(csv_headers)\n",
    "\n",
    "        writer.writerow(csv_data)\n",
    "\n",
    "    # Display the results using tabulate\n",
    "    if not os.path.exists(csv_filename):\n",
    "        print(f\"Error: {csv_filename} not found.\")\n",
    "    else:\n",
    "        with open(csv_filename, mode=\"r\", newline=\"\") as file:\n",
    "            reader = csv.reader(file)\n",
    "            data = list(reader)\n",
    "\n",
    "        if len(data) < 1:\n",
    "            print(\"CSV file exists but has no data to display.\")\n",
    "        else:\n",
    "            headers = data[0]\n",
    "            rows = data[1:]\n",
    "            print(tabulate(rows, headers=headers, tablefmt=\"grid\"))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    base_model = load_quantized_whisper_model()\n",
    "    model = apply_lora(base_model)\n",
    "    dataset = load_and_prepare_dataset()\n",
    "    feature_extractor, tokenizer, processor = load_processors(model_name_or_path, language, task)\n",
    "    prepare_dataset = get_prepare_dataset_fn(feature_extractor, tokenizer)\n",
    "    dataset = dataset.map(prepare_dataset, remove_columns=[\"audio\", \"sentence\"])\n",
    "\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    eval_dataset = dataset[\"test\"]\n",
    "\n",
    "    data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor)\n",
    "\n",
    "    training_args, trainer  = train_model(\n",
    "        model=model,\n",
    "        processor=processor,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        output_dir=output_dir,\n",
    "        seed=seed,\n",
    "        csv_filename=csv_filename\n",
    "    )\n",
    "\n",
    "\n",
    "    # Run evaluation with WER on the trained model\n",
    "    wer, normalized_wer = evaluate_model_wer(model, tokenizer, dataset[\"test\"], data_collator)\n",
    "    print(f\"WER: {wer:.2f}%, Normalized WER: {normalized_wer:.4f}%\")\n",
    "\n",
    "    # Logging\n",
    "    log_results_to_csv(wer, normalized_wer, dataset, seed, csv_filename=\"eval_test.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
