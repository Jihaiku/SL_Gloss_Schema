{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49a069ff-fdb1-4522-83d2-5444dbbcc344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing dataset...\n",
      "\n",
      "Sampling rate: 16000 Hz\n",
      "\n",
      "Dataset info:\n",
      "Dataset Name: mozilla-foundation/common_voice_11_0\n",
      "Percentage of the dataset: 0.005%\n",
      "Split: train\n",
      "  Number of samples: 4743\n",
      "Split: test\n",
      "  Number of samples: 81\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Dict, Union\n",
    "from datasets import load_dataset, Audio\n",
    "from transformers import WhisperFeatureExtractor, WhisperTokenizer, WhisperProcessor\n",
    "\n",
    "from config_0 import load_config\n",
    "# Load config\n",
    "config = load_config()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model_name_or_path = config[\"model_name_or_path\"]\n",
    "language = config[\"language\"]\n",
    "language_abbr = config[\"language_abbr\"]\n",
    "task = config[\"task\"]\n",
    "dataset_name = config[\"dataset_name\"]\n",
    "size = config[\"size\"]\n",
    "seed = random.randint(1, 200)\n",
    "\n",
    "def load_and_prepare_dataset():\n",
    "    # Here is where I gotta change things later\n",
    "    dataset = load_dataset(dataset_name, language_abbr) # <- This is where you should change it\n",
    "    dataset = dataset[\"train\"].train_test_split(train_size=size, seed=seed) # <- change size to 1.0\n",
    "    dataset[\"test\"] = load_dataset(dataset_name, language_abbr, split=\"test\").train_test_split(train_size=size, seed=seed)[\"train\"]\n",
    "\n",
    "    # Check required columns\n",
    "    required_columns = [\"audio\", \"sentence\"] #<- rename columns\n",
    "    missing_columns = {}\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        missing = [col for col in required_columns if col not in dataset[split].column_names]\n",
    "        if missing:\n",
    "            missing_columns[split] = missing\n",
    "\n",
    "    if missing_columns:\n",
    "        missing_str = \", \".join([f\"{split}: {', '.join(missing)}\" for split, missing in missing_columns.items()])\n",
    "        raise ValueError(\n",
    "            f\"Missing required columns in splits: {missing_str}. The dataset must contain 'audio' and 'sentence' columns in both 'train' and 'test'.\"\n",
    "        )\n",
    "\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        dataset[split] = dataset[split].remove_columns(\n",
    "            [col for col in dataset[split].column_names if col not in required_columns]\n",
    "        )\n",
    "\n",
    "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_processors(model_name_or_path, language, task):\n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name_or_path)\n",
    "    tokenizer = WhisperTokenizer.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "    processor = WhisperProcessor.from_pretrained(model_name_or_path, language=language, task=task)\n",
    "    return feature_extractor, tokenizer, processor\n",
    "\n",
    "\n",
    "# def prepare_dataset(batch):\n",
    "#     audio = batch[\"audio\"]\n",
    "#     batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
    "#     batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "#     return batch\n",
    "\n",
    "def get_prepare_dataset_fn(feature_extractor, tokenizer):\n",
    "    def prepare_dataset(batch):\n",
    "        audio = batch[\"audio\"]\n",
    "        batch[\"input_features\"] = feature_extractor(\n",
    "            audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]\n",
    "        ).input_features[0]\n",
    "        batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
    "        return batch\n",
    "    return prepare_dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Loading and preparing dataset...\")\n",
    "    dataset = load_and_prepare_dataset()\n",
    "\n",
    "    # Print sampling rate\n",
    "    sample_audio = dataset[\"train\"][0][\"audio\"]\n",
    "    print(f\"\\nSampling rate: {sample_audio['sampling_rate']} Hz\")\n",
    "    \n",
    "    print(\"\\nDataset info:\")\n",
    "    print(f\"Dataset Name: {dataset_name}\")\n",
    "    print(f\"Percentage of the dataset: {size}%\")\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        print(f\"Split: {split}\")\n",
    "        print(f\"  Number of samples: {len(dataset[split])}\")\n",
    "        # print(f\"  Column names: {dataset[split].column_names}\")\n",
    "        # print(f\"  First sample:\\n{dataset[split][0]}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
